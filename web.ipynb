{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09e87fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255fad1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "Scraping page 11...\n",
      "Scraping page 12...\n",
      "Scraping page 13...\n",
      "Scraping page 14...\n",
      "Scraping page 15...\n",
      "Scraping page 16...\n",
      "Scraping page 17...\n",
      "Scraping page 18...\n",
      "Scraping page 19...\n",
      "Scraping page 20...\n",
      "Scraping page 21...\n",
      "Scraping page 22...\n",
      "Scraping page 23...\n",
      "Scraping page 24...\n",
      "Scraping page 25...\n",
      "Scraping page 26...\n",
      "Scraping page 27...\n",
      "Scraping page 28...\n",
      "Scraping page 29...\n",
      "Scraping page 30...\n",
      "Scraping page 31...\n",
      "Scraping page 32...\n",
      "Scraping page 33...\n",
      "Scraping page 34...\n",
      "Scraping page 35...\n",
      "Scraping page 36...\n",
      "Scraping page 37...\n",
      "Scraping page 38...\n",
      "Scraping page 39...\n",
      "Scraping page 40...\n",
      "Scraping page 41...\n",
      "Scraping page 42...\n",
      "Scraping page 43...\n",
      "Scraping page 44...\n",
      "Scraping page 45...\n",
      "Scraping page 46...\n",
      "Scraping page 47...\n",
      "Scraping page 48...\n",
      "Scraping page 49...\n",
      "Scraping page 50...\n",
      "Scraping page 51...\n",
      "Scraping page 52...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nkris\\AppData\\Local\\Temp\\ipykernel_8056\\612589232.py:51: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"TV_Name\"].replace(\"\", np.nan, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# ----------------------------\n",
    "# 1. SCRAPING FUNCTION\n",
    "# ----------------------------\n",
    "def scrape_flipkart(page):\n",
    "    url = f\"https://www.flipkart.com/search?q=tv&page={page}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    def safe_extract(tag, class_name):\n",
    "        return [x.get_text(\" \", strip=True) for x in soup.find_all(tag, {'class': class_name})]\n",
    "\n",
    "    names   = safe_extract('div', 'KzDlHZ')\n",
    "    ratings = safe_extract('div', 'XQDdHH')\n",
    "    infos   = safe_extract('div', '_6NESgJ')\n",
    "    prices  = safe_extract('div', 'Nx9bqj _4b5DiR')\n",
    "    offers  = safe_extract('div', 'UkUFwK')\n",
    "    Deal_type  = safe_extract('div', 'yiggsN O5Fpg8')\n",
    "    Deal_type  = safe_extract('span', 'Wphh3N','span')\n",
    "    \n",
    "\n",
    "    \n",
    "    max_len = max(len(names), len(ratings), len(infos), len(prices), len(offers),len(Deal_type))\n",
    "    pad = lambda lst: lst + [\"\"] * (max_len - len(lst))\n",
    "\n",
    "    return list(zip(pad(names), pad(ratings), pad(infos), pad(prices), pad(offers),pad(Deal_type)))\n",
    "\n",
    "# ----------------------------\n",
    "# 2. SCRAPE ALL PAGES\n",
    "# ----------------------------\n",
    "all_data = []\n",
    "for page in range(1, 53):\n",
    "    print(f\"Scraping page {page}...\")\n",
    "    all_data.extend(scrape_flipkart(page))\n",
    "\n",
    "# ----------------------------\n",
    "# 3. CREATE DATAFRAME\n",
    "# ----------------------------\n",
    "df = pd.DataFrame(all_data, columns=[\"TV_Name\", \"Rating\", \"Info_Raw\", \"Price\", \"Offers\",\"Deal_type\"])\n",
    "df[\"TV_Name\"].replace(\"\", np.nan, inplace=True)\n",
    "df.dropna(subset=[\"TV_Name\"], inplace=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 4. CLEANING & FEATURE EXTRACTION\n",
    "# ----------------------------\n",
    "df['Resolution']    = df['Info_Raw'].str.extract(r'^(.*?)\\s*\\|')\n",
    "df['Panel_Type']    = df['Info_Raw'].str.extract(r'\\|\\s*(.*?)\\s*Model ID')\n",
    "df['Model_ID']      = df['Info_Raw'].str.extract(r'Model ID:\\s*(.*?)\\s*Launch')\n",
    "df['Launch_Year']   = df['Info_Raw'].str.extract(r'Launch Year:\\s*(\\d{4})').astype(float)\n",
    "df['Sound_Output']  = df['Info_Raw'].str.extract(r'Total Sound Output:\\s*(\\d+)\\s*W').astype(float)\n",
    "df['Warranty']      = df['Info_Raw'].str.extract(r'(Warranty.*)').fillna(\"No Warranty\")\n",
    "\n",
    "df['Tv_name']       = df['TV_Name'].str.extract(r'^(.*?)\\s*\\d+\\s*cm')\n",
    "df['Size_cm']       = df['TV_Name'].str.extract(r'(\\d+)\\s*cm').astype(float)\n",
    "df['Size_inch']     = df['TV_Name'].str.extract(r'\\((\\d+)\\s*inch').astype(float)\n",
    "\n",
    "df['Offers'] = df['Offers'].str.replace('%', '').str.replace('off', '').replace(\"\", \"0\").astype(int)\n",
    "df['Price']  = df['Price'].str.replace('â‚¹', '').str.replace(',', '').astype(float)\n",
    "\n",
    "# ----------------------------\n",
    "# 5. OS Type Extraction\n",
    "# ----------------------------\n",
    "def get_os(x):\n",
    "    x = x.lower()\n",
    "    for os in [\"google tv\", \"android tv\", \"webos\", \"tizen\", \"fire tv\", \"linux tv\", \"coolita tv\"]:\n",
    "        if os in x:\n",
    "            return os.title()\n",
    "    return \"Other\"\n",
    "\n",
    "df['OS_Type'] = df['TV_Name'].apply(get_os)\n",
    "\n",
    "products = df[['Tv_name','Resolution','Panel_Type','Launch_Year',\n",
    "               'Sound_Output','Warranty','Size_cm','Size_inch',\n",
    "               'OS_Type','Rating','Deal_type','Price']].drop_duplicates()\n",
    "\n",
    "final_df = df[[\n",
    "    \"TV_Name\", \n",
    "    \"Tv_name\",\n",
    "    \"Resolution\",\n",
    "    \"Panel_Type\",\n",
    "    \"Model_ID\",\n",
    "    \"Launch_Year\",\n",
    "    \"Sound_Output\",\n",
    "    \"Warranty\",\n",
    "    \"Size_cm\",\n",
    "    \"Size_inch\",\n",
    "    \"OS_Type\",\n",
    "    \"Rating\",\n",
    "    \"Deal_type\",\n",
    "    \"Price\",\n",
    "    \"Offers\"\n",
    "]]\n",
    "\n",
    "# Export as CSV\n",
    "final_df.to_csv(\"Flipkart_TV_Splitted_Cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6fccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_flipkart(page):\n",
    "    url = f\"https://www.flipkart.com/search?q=tv&page={page}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    def safe_extract(tag, class_name):\n",
    "        return [x.get_text(\" \", strip=True) for x in soup.find_all(tag, {'class': class_name})]\n",
    "\n",
    "    names      = safe_extract('div', 'KzDlHZ')\n",
    "    ratings    = safe_extract('div', 'XQDdHH')\n",
    "    infos      = safe_extract('div', '_6NESgJ')\n",
    "    prices     = safe_extract('div', 'Nx9bqj _4b5DiR')\n",
    "    offers     = safe_extract('div', 'UkUFwK')\n",
    "\n",
    "    # ðŸ‘‰ UPDATED: extract tUxRFH only\n",
    "    Deal_type  = safe_extract('div', 'tUxRFH')\n",
    "    \n",
    "    max_len = max(len(names), len(ratings), len(infos), len(prices), len(offers), len(Deal_type))\n",
    "    pad = lambda lst: lst + [\"\"] * (max_len - len(lst))\n",
    "\n",
    "    return list(zip(\n",
    "        pad(names), pad(ratings), pad(infos), pad(prices), pad(offers), pad(Deal_type)\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
